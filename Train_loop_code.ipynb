{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Connecting Google Drive"
      ],
      "metadata": {
        "id": "bPYUM0N4cbWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XHx7sMFJH-B4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating virtual environment"
      ],
      "metadata": {
        "id": "ZGYS6T8Kcime"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.9/site-packages')"
      ],
      "metadata": {
        "id": "f_893GsV6w3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!conda create -y -n py36 python=3.6.9\n",
        "\n",
        "!source activate py36"
      ],
      "metadata": {
        "id": "TWMrHWi_7pmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!conda install -y -n py36 -c pytorch pytorch=1.10.0 torchvision=0.11.0\n",
        "\n",
        "# !conda run -n py36 pip install pycocotools"
      ],
      "metadata": {
        "id": "YWaqyB0y8MrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!conda run -n py36 python --version\n",
        "\n",
        "!conda run -n py36 pip list"
      ],
      "metadata": {
        "id": "Z7U5nPND9lWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installs and Imports"
      ],
      "metadata": {
        "id": "TNQr63dXco7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchmetrics --quiet"
      ],
      "metadata": {
        "id": "-xi2QDMIdUk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models.detection import ssdlite320_mobilenet_v3_large\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from pycocotools.coco import COCO\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "# from torchmetrics.detection.mean_ap import MeanAveragePrecision   # torch metric will not work with older pytorch versions"
      ],
      "metadata": {
        "id": "W8vIE1X58_B6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating dataset"
      ],
      "metadata": {
        "id": "p8tkr2BJctGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DroneDataset(Dataset):\n",
        "    def __init__(self, root, transforms=None):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        self.imgs = [img for img in os.listdir(root) if img.endswith(\".JPEG\")]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_name = self.imgs[index]\n",
        "        img_path = os.path.join(self.root, img_name)\n",
        "        txt_path = os.path.join(self.root, os.path.splitext(img_name)[0] + \".txt\")\n",
        "\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        width, height = img.size\n",
        "\n",
        "\n",
        "        with open(txt_path, 'r') as f:\n",
        "            line = f.readline().strip()\n",
        "            category_id, x_center, y_center, box_width, box_height = map(float, line.split())\n",
        "\n",
        "            xmin = x_center - (box_width / 2)\n",
        "            ymin = y_center - (box_height / 2)\n",
        "\n",
        "            xmin *= width\n",
        "            ymin *= height\n",
        "            xmax = xmin + box_width * width\n",
        "            ymax = ymin + box_height * height\n",
        "\n",
        "            boxes = [[xmin, ymin, xmax, ymax]]\n",
        "            labels = [int(category_id)]\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target['boxes'] = boxes\n",
        "        target['labels'] = labels\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "metadata": {
        "id": "LEJ12A41qnHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Resize([320, 320])\n",
        "])\n",
        "\n",
        "train_dataset = DroneDataset(root='/content/drive/MyDrive/drone/train', transforms=transform)\n",
        "val_dataset = DroneDataset(root='/content/drive/MyDrive/drone/val', transforms=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)), drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)), drop_last=True)"
      ],
      "metadata": {
        "id": "AiFwX9ndBQTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing model"
      ],
      "metadata": {
        "id": "2aX2kWa-c9ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = ssdlite320_mobilenet_v3_large(pretrained=False, num_classes=2)\n",
        "model = ssdlite320_mobilenet_v3_large(pretrained=True)\n",
        "model.head.classification_head.num_classes = 2\n",
        "print(model)"
      ],
      "metadata": {
        "id": "FGVePXKUL8nY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0002)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "# metric = MeanAveragePrecision()"
      ],
      "metadata": {
        "id": "hYsGrJ3XBXuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Loop"
      ],
      "metadata": {
        "id": "mti4-LG8dCQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, data_loaders, device, num_epochs):\n",
        "\n",
        "  loss_hist_train = [0] * num_epochs\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    print(f\"Epoch {epoch}\")\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for images, targets in tqdm(data_loaders['train']):\n",
        "      images = list(image.to(device) for image in images)\n",
        "      targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "      loss_dict = model(images, targets)\n",
        "      losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "      loss_hist_train[epoch] += losses.item() * len(targets)\n",
        "\n",
        "      losses.backward()\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    epoch_loss = loss_hist_train[epoch] / len(data_loaders['train'].dataset)\n",
        "    print(f\"Train Loss: {epoch_loss}\")\n",
        "\n",
        "    # model.eval()\n",
        "\n",
        "    # for images, targets in tqdm(data_loaders['val']):\n",
        "    #   images = list(image.to(device) for image in images)\n",
        "    #   targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "    #   outputs = model(images)\n",
        "\n",
        "    #   for target, output in zip(targets, outputs):\n",
        "    #     metric.update([{'boxes': output['boxes'], 'scores': output['scores'], 'labels': output['labels']}], [target])\n",
        "\n",
        "    # mAP = metric.compute()\n",
        "    # print(f\"Mean Average Precision: {mAP['map']}\")\n",
        "    # metric.reset()"
      ],
      "metadata": {
        "id": "GHHwGOAbBp2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating model performance"
      ],
      "metadata": {
        "id": "p5NcqNyIdF5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model.eval()\n",
        "\n",
        "# for images, targets in tqdm(val_loader):\n",
        "#   images = list(image.to(device) for image in images)\n",
        "#   targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "#   outputs = model(images)\n",
        "\n",
        "#   for target, output in zip(targets, outputs):\n",
        "#     # predicted_box = torch.argmax(output['scores'])\n",
        "#     # p_labels = output['labels'][predicted_box]\n",
        "#     # p_boxes = output['boxes'][predicted_box].unsqueeze(0)\n",
        "#     # p_scores = output['scores'][predicted_box]\n",
        "#     # new_output = [{'boxes': p_boxes, 'scores': p_scores, 'labels': p_labels}]\n",
        "#     # metric.update(new_output, [target])\n",
        "\n",
        "#     p_boxes = output['boxes']\n",
        "#     p_labels = output['labels']\n",
        "#     p_scores = output['scores']\n",
        "#     new_output = [{'boxes': p_boxes, 'scores': p_scores, 'labels': p_labels}]\n",
        "#     metric.update(new_output, [target])\n",
        "\n",
        "# mAP = metric.compute()\n",
        "# print(f\"Mean Average Precision: {mAP['map']}\")\n",
        "# metric.reset()"
      ],
      "metadata": {
        "id": "mDRjGJvplpUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "data_loaders = {'train': train_loader, 'val': val_loader}\n",
        "\n",
        "train(model, optimizer, data_loaders, device, num_epochs)"
      ],
      "metadata": {
        "id": "o-XlXxw_tDix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lr_scheduler.step()\n",
        "\n",
        "# torch.save(model, f'model_epoch_{epoch}.pth')\n",
        "# print(f\"Model saved at epoch {epoch}\")\n",
        "\n",
        "torch.save(model, '/content/drive/MyDrive/drone_detection_ssd_model_updated.pth')\n",
        "# print(\"Model training complete and saved.\")"
      ],
      "metadata": {
        "id": "cPKAyWq27GBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking on validation set"
      ],
      "metadata": {
        "id": "VDUEFahs1t2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('/content/drive/MyDrive/drone_detection_ssd_model_updated.pth')"
      ],
      "metadata": {
        "id": "j-Aco4-q7N5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "\n",
        "model.eval()\n",
        "\n",
        "for images, targets in tqdm(val_loader):\n",
        "\n",
        "    images = list(image.to(device) for image in images)\n",
        "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "    for image, target in zip(images, targets):\n",
        "\n",
        "      p_t = model(image.unsqueeze(0))\n",
        "      predicted_box = torch.argmax(p_t[0]['scores'])\n",
        "\n",
        "      p_labels = p_t[0]['labels'][predicted_box]\n",
        "      p_boxes = p_t[0]['boxes'][predicted_box].unsqueeze(0)\n",
        "\n",
        "      t_boxes = target['boxes']\n",
        "\n",
        "      img_dtype_converter = transforms.ConvertImageDtype(torch.uint8)\n",
        "      img = img_dtype_converter(image)\n",
        "\n",
        "      annotated_image = draw_bounding_boxes(img, t_boxes, [str(p_labels.item())], colors=\"yellow\", width=2)\n",
        "      annotated_image = draw_bounding_boxes(annotated_image, p_boxes, [str(p_labels.item())], colors=\"red\", width=2)\n",
        "\n",
        "      fig, ax = plt.subplots()\n",
        "      ax.imshow(annotated_image.permute(1, 2, 0).numpy())\n",
        "      ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "      fig.tight_layout()\n",
        "      plt.show()\n",
        "    break\n"
      ],
      "metadata": {
        "id": "oxrPs-Bjm5xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End"
      ],
      "metadata": {
        "id": "noklpsSQ1ktM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "\n",
        "model.eval()\n",
        "\n",
        "for images, targets in tqdm(val_loader):\n",
        "    images = list(image.to(device) for image in images)\n",
        "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "    idx = 0\n",
        "\n",
        "    p_t = model(images)\n",
        "    confidence_length = len(torch.where(p_t[idx]['scores'] > 0.5)[0])\n",
        "    predicted_box = torch.argmax(p_t[idx]['scores'])\n",
        "\n",
        "    p_labels = p_t[idx]['labels'][predicted_box]\n",
        "    p_boxes = p_t[idx]['boxes'][predicted_box].unsqueeze(0)\n",
        "\n",
        "    print(len(targets))\n",
        "    print(images[0].shape)\n",
        "\n",
        "    t_boxes = targets[0]['boxes']  # Assuming single target for the example\n",
        "    # x_min, y_min, x_max, y_max = t_boxes\n",
        "    # x_min, x_max = x_min * images[0].shape[2], x_max * images[0].shape[2]\n",
        "    # y_min, y_max = y_min * images[0].shape[1], y_max * images[0].shape[1]\n",
        "    # t_boxes = torch.tensor([[x_min, y_min, x_max, y_max]])\n",
        "    print(t_boxes)\n",
        "\n",
        "    img_dtype_converter = transforms.ConvertImageDtype(torch.uint8)\n",
        "    img = img_dtype_converter(images[idx])\n",
        "\n",
        "    annotated_image = draw_bounding_boxes(img, t_boxes, [str(p_labels.item())], colors=\"yellow\", width=2)\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.imshow(annotated_image.permute(1, 2, 0).numpy())\n",
        "    ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "    break\n"
      ],
      "metadata": {
        "id": "HNV3_HDXP54g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.utils import draw_bounding_boxes\n",
        "\n",
        "model.eval()\n",
        "\n",
        "for images, targets in tqdm(val_loader):\n",
        "  images = list(image.to(device) for image in images)\n",
        "  targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "  idx = 0\n",
        "\n",
        "  p_t = model(images)\n",
        "  confidence_length = len(torch.where(p_t[idx]['scores'] > 0.5)[0])\n",
        "  predicted_box = torch.argmax(p_t[idx]['scores'])\n",
        "\n",
        "  p_labels = p_t[idx]['labels'][predicted_box]\n",
        "  p_boxes = p_t[idx]['boxes'][predicted_box].unsqueeze(0)\n",
        "\n",
        "  print(images[0].shape)\n",
        "\n",
        "  x_min, y_min, x_max, y_max = targets[0]['boxes'][0]\n",
        "  x_min, x_max = x_min * images[0].shape[2], x_max * images[0].shape[2]\n",
        "  y_min, y_max = y_min * images[0].shape[1], y_max * images[0].shape[1]\n",
        "  t_boxes = torch.tensor([[x_min - 20, y_min-20, x_max-20, y_max-20]])\n",
        "\n",
        "\n",
        "  img_dtype_converter = transforms.ConvertImageDtype(torch.uint8)\n",
        "  img = img_dtype_converter(images[idx])\n",
        "\n",
        "  annotated_image = draw_bounding_boxes(img, t_boxes, [str(p_labels.item())], colors=\"yellow\", width=2)\n",
        "  annotated_image = draw_bounding_boxes(annotated_image, p_boxes, [str(p_labels.item())], colors=\"red\", width=2)\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.imshow(annotated_image.permute(1, 2, 0).numpy())\n",
        "  ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "  fig.tight_layout()\n",
        "  fig.show()\n",
        "  break"
      ],
      "metadata": {
        "id": "a11ZA71TSSnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5XxnDxMVBz_j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}